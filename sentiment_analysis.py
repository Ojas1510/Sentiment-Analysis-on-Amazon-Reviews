# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TprF37DfoNmpGw6yoVc534EGr_qN4N3J

# Sentiment Analysis
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('ggplot')
import nltk
nltk.download('vader_lexicon')

df=pd.read_csv('/content/drive/MyDrive/Data/Reviews.csv')

df.head()

df.shape

df.describe

# Data info:
df.info()

# Checking for the missing values
print(df.isnull().sum())
df.dropna(axis = 0)

"""# Exploratory Data Analysis"""

ax = df['Score'].value_counts().sort_index() \
    .plot(kind='bar',
          title='Count of Reviews by Stars',
          figsize=(10, 5))
ax.set_xlabel('Review Stars')
plt.show()

"""From this bar chart it can be concluded that majority of reviews are positive ( 5 stars)

# Using Natural Language Toolkit (NLTK)

NLTK can be used for tokenization , Parts of Speech tagging , stemming, lemmatization, parsing, and sentiment analysis ,etc.

## Using VADER Model

VADER( Valence Aware Dictionary for Sentiment Reasoning) is an NLTK module that provides sentiment scores based on the words used. It is a rule-based sentiment analyzer in which the terms are generally labeled as per their semantic orientation as either positive or negative. <br>
This uses a "bag of words" approach:
1. Stop words are removed
2. each word is scored and combined to a total score.
"""

# Importing VADER Sentiment Scoring
from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
sia = SentimentIntensityAnalyzer()

print( sia.polarity_scores('I am happy') )
print( sia.polarity_scores('I am very happy') )
print( sia.polarity_scores('I am sad')  )

"""VADER Model predicts the sentiment polarity scores with four values: <br>
    neg (Negative): It ranges from 0-1 with 0 being lest negatib=ve and 1 being most negative. <br>
    new (Neutral): It ranges from 0-1 ,indicatring neytrality of the sentence. <br>
    pos (Positive): It ranges from 0-1 ,with 0 being leadt positive and 1 being most positive. <br>
    Compound: It is compound total of all three scores and ranges from -1 to 1. <br>    
"""

# Plarity scoring of the entire dataset
res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    text = row['Text']
    myid = row['Id']
    res[myid] = sia.polarity_scores(text)

#Concatinating the sentiment score of each row with orignal DataFrame
#Making a new dataframe called vaders
vaders = pd.DataFrame(res).T
vaders = vaders.reset_index().rename(columns={'index': 'Id'})
vaders = vaders.merge(df, how='left')

vaders.head()

"""## Ploting VADER result"""

vader_result = sns.barplot(data=vaders, x='Score', y='compound')
vader_result.set_title('Compund Score by Amazon Star Review')
plt.show()

fig, vader_result  = plt.subplots(1, 3, figsize=(12, 3))
sns.barplot(data=vaders, x='Score', y='pos', ax=vader_result[0])
sns.barplot(data=vaders, x='Score', y='neu', ax=vader_result[1])
sns.barplot(data=vaders, x='Score', y='neg', ax=vader_result[2])
vader_result[0].set_title('Positive')
vader_result[1].set_title('Neutral')
vader_result[2].set_title('Negative')
plt.tight_layout()
plt.show()

"""## Using Roberta Pretrained Model

Pretrained model on English language using a masked language modeling (MLM) objective. <br>
RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion <br>
Transformer model accounts for the words but also the context related to other words. <br>
This model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks
"""

# Importing AutoTokenizer and other libraries
!pip install transformers
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

# Runing Roberta Model
def polarity_scores_roberta(text):
    encoded_text = tokenizer(text, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

df=df[:5000]
res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        text = row['Text']
        myid = row['Id']
        vader_result = sia.polarity_scores(text)
        vader_result_rename = {}
        for key, value in vader_result.items():
            vader_result_rename[f"vader_{key}"] = value
        roberta_result = polarity_scores_roberta(text)
        both = {**vader_result_rename, **roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')

results_df = pd.DataFrame(res).T
results_df = results_df.reset_index().rename(columns={'index': 'Id'})
results_df = results_df.merge(df, how='left')

"""## Comparing Scores between models"""

results_df.columns

"""## Combining and comparing the results"""

sns.pairplot(data=results_df,
             vars=['vader_neg', 'vader_neu', 'vader_pos',
                  'roberta_neg', 'roberta_neu', 'roberta_pos'],
            hue='Score',
            palette='tab10')
plt.show()

"""# Using the Transformers Pipeline"""

from transformers import pipeline

sent_pipeline = pipeline("sentiment-analysis")

sent_pipeline('I love food')

sent_pipeline('Food was very bad ,but i still liked it')

sent_pipeline('quality was not good')

